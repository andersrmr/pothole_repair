14 Sept 2015

AM

Found online examples of geocoding by street intersetion, one involving googlemaps API - yeay!

https://groups.google.com/forum/#!topic/google-maps-api-web-services/UhvCp2yyx-M

https://developer.here.com/rest-apis/documentation/geocoder/topics/example-geocode-find-street-intersection.html

http://blog.batchgeo.com/geocoding-intersections/

Noon

Excceeded my rquests quota at GoogleV3 geocoder before getting what I needed.  Before even debugging the code.  Trying to get API key to see if I can do more querying that way.

AIzaSyBpxr2Rx1WbYqrxpuUg_I819YuDw0Pvm_0

3:30

OK, I finally have mapped the first 500 potholes/rows in my Seattle dataset.

Adam gave me a tip that I can just submit my locations to batchgeo and it will plot them all for me.

I need to talk to Ben again about how to structure my repo properly so that I can commmit daily.

Also, for my API query limit problem, Ben suggested I might create multiple accounts, i.e., work in parallel with multiple keys


A few of the Installs from today:

http://deparkes.co.uk/2015/01/29/install-shapely-on-anaconda/
https://pypi.python.org/pypi/Shapely
fiona: http://toblerity.org/fiona/manual.html
http://pysal.readthedocs.org/en/latest/users/installation.html

Midnight

I have code that plots potholes on Seattle neighborhood map and shows density using chloropleths.  Locations seem reasonably good.  I still don't have geocodes for all the potholes.  Also, I still don't know how to include a third "feature" on the plot, e.g., represent time to repair, say, by size of hexagons/bubble plot.

One pattern is clear so far: locations/density of potholes

Scale is going to be a problem.  Would have to zoom in to capture the length scale of a pothole.

Also, I appear to decline from 482 city points/potholes to plotting only 121 potholes

Plan for Tomorrow:

0. Study geopy examples on train.

1. Continue geo-coding pothole locations first thing.  Try to fill query quota for each 24 hr period.  Add geo-coded street address to df also so that I can maybe use it with zillow to get fine scale property value.  also need to geo-code some important locations, e.g., center of ea Seattle city neighborhood, Space Needle, Pike Place Market, center of City of Seattle

- Census data
- zillow research data
- scrape from zillow
1.create url based on a small box around each pothole location/geocode
2.Grab the html
3.Search for the home values within that bounding box, i.e., grab everything that looks like a property value

2. Clean Seattle data completely and save cleaning code (later, can create class)

3. Plot time series of time to repair potholes and quantity of potholes

4. Get socio-economic data per pothole, either from zillow or census or older city data.

- Reverse geocode to get street addresses.  This seems to have worked.
- 

5. Plot/overlay time to repair on maps

(Wed: Get other basic feature data, e.g., road condition/weather)


15 Sept 2015

AM

Used reverse geocode to obtain actual street addresses "closest to each pothole".  Pickled new dataframe.

Need to remember to utilize my quota for today's geocodes.  Still should have about 2,000 queries left out of 2500.

PM

Starting new notebook to get socio-economic data from zillow.  Talked to Michael Hood about possible approaches.

div class = 'property-listing data'
  div class = 'property-info'
    dt class='zestimate' Foreclosure Est:
      $224K

Scraping was a disaster.  I couldn't get anything to work.  At end of day, Adam suggested how to get census data on property values at the level of a block group at factfinder.census.gov.  I can hopefully associate potholes with the block group they fall within.  May be problems with aligning projections and/or other problems.

Will suggested that I first look at how variable are the repair times by pothole location.  Depending on the variability, I may not need property values at the scale of an individual pothole.

Plan for Tomorrow:

Overall: Get all initial features ready by end of day to start with simplest model Thurs AM, e.g., street condition, socio-economics, distances,...

Blocker: Inability to geocode all my potholes due to API query limits

Blocker: Inability to get socio-economic data at level of individual pothole because of data unavailability and/or difficulty of scraping it.

For example, Beautifulsoup at Zillow doesn't work.

1. Figure out how to do bubble plot of geographical distribution of pothole repair times.

2. geolocate 1st 2,000 rows.

3. Add features of distances to major/central locations, e.g., Space Neeedle, centers of ea neighborhood district (I could do a visual estimate and get it off google maps), center of Seattle, downtown convention center.

4. Try to get data on pre-existing street condition, DoT

5. Back to Socio-economics


16 Sept 2015

AM

(Almost) Figured out how to make the scatter plots.  I'm close enough that I think I can do it once I get the data ready.

PM

I've decided I'm going to go back to the cleaned data set and use today's request/query quota to geocode the first 1,000 points/potholes.  Then proceed with those points, work on features, including socio-economic data and distances, and try to be in position to model by tomorrow AM.

Due to error, only got first 757.

Due to another Series index error, it took a long while to get the first 757 geo-coded records.

Ben suggested NLP on the LOCATION column to see if it might be predictive. 

I also need to be sure to create a feature for the neighborhood the pothole is in.  This should not be difficult with the mapping code.

Additional ideas for features available from data.seattle.gov GIS shapefile datasets, https://data.seattle.gov/dataset/data-seattle-gov-GIS-shapefile-datasets/f7tb-rnup

Projection issues, http://geoinformaticstutorial.blogspot.com/2012/10/reprojecting-shapefile-with-gdalogr-and.html

http://matplotlib.org/basemap/users/mapsetup.html

http://gis.stackexchange.com/questions/27702/what-is-the-srid-of-census-gov-shapfiles

1. Traffic signal distances
2. City of Seattle Parks
3. Street Network Database, size of roads
4. From contour map I can get slope/elevation
5. Locations of Traffic circles

Late PM

I got the neighborhood membership and several distance features working.

Also a "seasonality" category (actually, quarter of yr) and months till end of FY (assuming June.)

Can't figure out how to scale up map bubbles by a constant factor.  I just want them bigger. (http://glowingpython.blogspot.com/2011/11/how-to-make-bubble-charts-with.html)

https://stevendkay.wordpress.com/2009/10/12/scatter-plots-with-basemap-and-matplotlib/

http://basemaptutorial.readthedocs.org/en/latest/plotting_data.html#scatter



17 Sept 2015

Plan for Today

Meet with Ben to do git versioning set-up

Figure out how to expand my bubbles on bubble plot

Book some time with Giovanna to try to scrape zillow

Identify a way forward to get the rest of the data

Produce my initial models based on the features I have.

- LR, Decision Tree, Random Forest, Gradient Boosted

Tomorrow

Get the socio-economic data somehow

AM

Ming helped me get the bubble chart to work.

PM

Got my MVP/baseline model working, R^2 with basic Random Forest = 0.4

Colin, Ming suggested .DoY to transform the datetime variable.  Need to add this to the create_seasonality_feature script(s).

Couldn't get my horiz bar plot of importances to display hte name of the feature.  See http://stackoverflow.com/questions/9626298/matplotlib-chart-creating-horizontal-bar-chart

4:00 PM

Spend rest of day doing EDA on new features for a bit, then trying to get socio-economic info.

Evan found a link to building permit data that might be a surrogate for the housing data at https://data.seattle.gov/Permitting/Building-Permits-Current/mags-97de.  There should definitely be a way to get socio-economic data here, say, using a clustering approach to determine which records to represent a pothole.  Ben suggests this is too far a stretch.

Plan for Tomorrow

Try to get the rest of the pothole data

Get more features

Feature engineering

Plan for Weekend

Clean up my code, transfer out of IPython notebooks


18 Sept 2015

AM

My progress has really slowed down.  I downloaded free GIS viewer to try to find the unique/identifying fields in my block map shapefile.  The block map shapefile covers all WA.  The income information is in a separate csv file.  I think it has a corresponding identifier field.  First, I have to figure out which fields the potholes are in.  Then I would have to join the information in order to get the income associated with it.  This could work.  The WA shapefile is too big.  I cannot handle/process that entire shapefile in memory.  I'm stuck.

PM

Talked to Zach.  Recommended basemap tutorial

Notes

Ideas from Mark/Adam:

1. Get membership by street address.  Adam says there is an API that allows batch submission of a csv file with street addresses.

2. Go through and create a set of the block grouup memberships of each pothole.  Only if that membership is greater than 1 do I have a problem.

3. Spin up EC2 instances to get that job done, overcoming the apparent memory limitation of my machine.

4. Recall Zach saying we can also do the Amazon EC2 instances if necessary to get the geocoding done.

5. Mark also suggested colors instead of size to categorize reepair times, as those who cluster by eye better than sizes.

Other ideas:

1. Keep learning about basemap

2. Go through the QGIS menu and figure out what it can do.

3. Pull up additional Seattle and Tacoma data in QGIS and see if I may be able to pull/infer additional info about it, e.g., elevations, ***associations with street types***.


20 Sept 2015

Over weekend I got the census socio-economic data and covnerted the create features code from ipython notebook to py files.

Plan for Tomorrow

Convert all cleaning and geocoding code to py files.
Spin up EC2 instances to geocode all pothole data for Seattle.
NLTK on location column
Get DEM data and figure out how to do the road data, i.e., closeness to pothole locations.


21 Sept 2015

AM

Converted all cleaning and geocoding code to py files

PM

Set up 2 EC2 instances and confirmed that I can access the google API from them.  Also, figured out how to scp into the EC2 instances.

Ran the first 2000 rows of pothole data for Seattle.  I can run another ~2500 after midnight, independent of whether I get EC2 instances going.  It looks like I should not try to run more than 500 at a time.  I have another 400 or so left for today.  I should use them before midnight.

Plan for Tonight

2. Split up job on EC2 instances and get all of my data.  
Need separate API keys for each of the EC2 instances I run.

3. Also do another 2500 locally after midnight.

1. Also, put together (stack, rowbind) the different pieces of geocoded rows then run the entire pipeline to generate featured, cleaned data ready to submit to model.

Plan for Tomorrow

Get street-related and DEM-related features so that Wednesday I can do the serious modeling that is required to get a result.

Aside: Another interesting thing to try to predict/forecast is the pattern of occurence of potholes.  Is there a seasonal pattern?  When do potholes take place?


22 Sept 2015

AM

Worked on getting my data pipeline ready and geocoded through first 4,000 records.

Adam explained that using multiple EC2 instances to query google map API won't work because queries are linked to your google ID not the machine IP.

PM

Talked to Giovanna, who is also stuck on getting the zpids of the homes in the lat-long window.  She'll do a little more looking; suggested I not spend more than another 1/2 hour on it today.

Spend rest of afternoon getting shapefile/features from tiger lines/census that I can use to describe/differentiate between potholes.

1. Primary and Secondary Roads (by state)
2. Secondary School District (by state)
3. Unified School District (by state)
4. Elementary School District

Got better road, elevation data from data.seattle.gov
https://data.seattle.gov/dataset/data-seattle-gov-GIS-shapefile-datasets/f7tb-rnup

Two ways to use road data:

1. Create shapefile out of potholes and do the closest distances in QGIS.  (I definitely need to port the potholes to a shapefile and overlay in GIS for cool maps.)

2. Query road fields in QGIS and do the closest distance using Python.

Possible useful road descriptors (all categorical variables):

SND_FEACOD
ST_CODE
SEGMENT_TY
DIVIDED_CO
VEHICLE_US

Aside: Two nice outputs would be interesting: 

1. Prediction of abundance of potholes.  
2. Prediction of repair times to fix.

The INTPTLAT (interpolated), INTPTLON fields of the block group layer as the lat-longs of the centroid of each block group.  I could do closest distance to each pothole for block groups that have this field value in order to avoid NaNs in the value field.

This may be worth doing especially if econ values turns out to be a very predictive variable.  (Or even if not!)

OK, so I want

http://stackoverflow.com/questions/24415806/coordinate-of-the-closest-point-on-a-line
http://gis.stackexchange.com/questions/396/nearest-neighbor-between-a-point-layer-and-a-line-layer


23 Sept 2015

AM

Wanted to quickly get distance method working to determine closest street feature to each pothole.  Ran out of memory when tried to load large tuple of street objects and their features.

Figured out that MultiPolygon and polys created by shape method in Shapely differ only in that MultiPolygon is a shapely object and polys is a list of shapely objects.  Need to figure out how this is going to influence my analysis, in particular, my attempts to identify closest distance.

Now I've run into another issue of whether or not to remove potholes that do not fall within neighborhood and/or block group boundaries.  It seems better to leave them in, and just allow the functions to fail to assign a neighborhood label and or other features, giving NaNs instead.  At the end i can drop them from the modeling dataset. And from the map.  I would have to change the code to do this.

PM

Wanted to be modeling by now.  But still struggling with above issues.

Questions for Giovanna' code review:

1. Would you recommend classes and if so, what are recommendations for structure?
2. Would you recommend assigning empty strings for missing values when no neighborhood or economic value or something else?


24 Sept 2015

AM

Ran get_closest_distance function overnight.  Took over 3.5 hours.  ***Code stopped running about 7:30, just before I had to leave for the train.***

Ran clean_prep_before_model this morning.  Thought I had it clean, but got "could not convert string to float" error.  ***Just happened to look at the last row and saw that there is still an empty string in the neighborhood_label column***


Suggestions from Ben:

duration models

measures of performance, metrics
RMSE, MAPE, MAE

grid search on regularization parameters.
Look at residuals, plot against fitted values

Need to cast categoricals as dummies, check this, even for regression?


Notes on presentations from Giovanna:

1. repo: Sent to employers Tuesday AM
README, put slide deck in there.  summary of results, redundant with presentation
HIGH LEVEL, TECHNICAL DETAILS
GUIDE TO YOUR CODE, GIVE DETAILED INSTRUCTIONS
INSTRUCTIONS ON HOW TO RUN
Could provide access to data on S3.  Either case, nice to have instructions.

2. presentation: 4 min, 1 questions
summary of what you did, not proportional to time spent.  1st 20 secs: what is my project?  Most time: results, what I learned, conclusions, how I solved problem, what failed; at the end, a little on future work; data science not coding specifics. about 5 slides.  business casual, comfort, hair cut

3. app

do app before README

4. next week's schedule

Mon: another practice, Giovanna makes presentation order
Tues: repos go out, Lee will tell us who the companies are.
Wed: dress rehearsal

ProKarma
Amazon
9 interviewees
13 interviewers
7 min speed interviews after lunch
know the ML tool that u used
career goals, what do I want in a company


Questions for Zach before he leaves:

1. How to address outlier target points?  Should I display?
2. Do I need to dummy variables for regression?  yes, but..
3. Which model to try next?
4. What to focus on in presentation?
e.g., T-S prediction, model performance, important features, cross validation, grid search

(with order of importance)

2. Add category for NaNs for street features in order to recover a lot of my data.  DONE
Replace ea entry in a column with its z-score.  May be spreading my neighborhood data too thin if I replace them all with dummies.  Could create a False/True flag to do this step.

4. recast as binary classification: long vs. short repair times.

6. linear model first, then random forest regression

5. EDA with descriptive plots of duration (maps).  A good example: here's region a where we have econ data x, here's b with econ data y, and look at difference in repair times.  e.g., two maps side by side one showing econ data, one showing repair times

7. suppose I build simple decision tree and see what splits make the most difference in outcomes.

In a good talk, the model almost becomes secondary.  it's the discussion and the insights that matter.  Zach himself is very interested in what I find out.

1. check counts of zeros remaining.  DONE

ALSO:

Should not be difficult to add the precip, temp data once it is cleaned.  Make the date/time column into an index and add in the values that correspond to that index.

Tonight

CREATE 5 SLIDES.  Decide on the train what's going to be on the slides.
Keep in mind that I could do maps of any of my variables.

Remember feature importances code from Adam pair.

(Actually, it would not be difficult to do the time series thing.  Just do a groupby on the day and keep the counts of potholes initiated on each day.  The real issue is that there's less of a story there.)

3. No, the real issue is that number of potholes could be more predictive of repair time.  I have to check this.  DONE


SLIDE 1

Introduce issue: Plots of pothole abundance in Seattle.

SLIDE 2

Possible patterns.  Many stakeholders:
1. WA cascade cyclists, 
2. citizens like my neighbor who may have questions, 
3. city leaders who might wish to know how to prioritize reources
4. data scientists

SLIDE 3

A number of features were investigated

SLIDE 4

Result: Here's what we found.  

SLIDE 5

Here's what we learned

SLIDE 6

Other datasets available and coming online should enable us to investigate to what extent these causal factors are universal.

What data science/modeling  steps might I try next to attempt this?



















